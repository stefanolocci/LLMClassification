{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "2ClSVqcd36zf",
        "PFzoQmW63-5t",
        "qtaN63jVWt8l",
        "01U52UI4riEM",
        "RkuftVN05wQQ",
        "vBJKcja345iD"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Classification on EcoVerse Tasks Metrics and Utils\n"
      ],
      "metadata": {
        "id": "1_r5ZF6kEfEx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "QkstvMPi3AM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name='gemma-2b-it'\n",
        "#model_name='Llama-2-13b-chat-hf'\n",
        "#model_name='gpt-3.5-turbo-0125'"
      ],
      "metadata": {
        "id": "Pmz6T41hs7FY"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for iteration in range(1, 101):\n",
        "    # Define the paths\n",
        "    input_file_path = f'/content/drive/MyDrive/EcoVerse_Results/{model_name}/stance/results_{iteration}.txt'\n",
        "    modified_file_path = f'/content/drive/MyDrive/EcoVerse_Results/{model_name}/stance/formatted_results/results_{iteration}.txt'  # Change this if you want to overwrite the original file\n",
        "\n",
        "    # Read the file, replace </s> with </s>\\n, and save the modified content\n",
        "    with open(input_file_path, 'r', encoding='utf-8') as file:\n",
        "        content = file.read()\n",
        "    # Remove all newline characters\n",
        "    #content = content.replace('*', ' ')\n",
        "    content = content.replace('\\n', '')\n",
        "\n",
        "    # Add a newline character after each </s>\n",
        "    content = content.replace('ID:', '\\nID:')\n",
        "    # Remove the first newline character\n",
        "    content = content[1:]\n",
        "\n",
        "    with open(modified_file_path, 'w', encoding='utf-8') as file:\n",
        "        file.write(content)\n",
        "\n",
        "print(\"Modification completed.\")\n"
      ],
      "metadata": {
        "id": "MghbWPHL85cq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_probability(prob_str):\n",
        "    # Check if the probability is expressed as a percentage\n",
        "    if prob_str == '0.3.':\n",
        "      prob_str = '0.3'\n",
        "    if prob_str == '0.0.':\n",
        "      prob_str = '0.0'\n",
        "    if '%' in prob_str:\n",
        "        # Remove the '%' sign and divide by 100\n",
        "        return float(prob_str.replace('%', '')) / 100\n",
        "    else:\n",
        "        # Convert the probability to a float\n",
        "        prob_float = float(prob_str)\n",
        "        # If the probability is an integer from 1 to 100, divide by 100\n",
        "        if prob_float > 1:\n",
        "            return prob_float / 100\n",
        "        # Otherwise, return the probability as is\n",
        "        return prob_float"
      ],
      "metadata": {
        "id": "xEywjIEM2MEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "\n",
        "for iteration in range(1, 101):\n",
        "    # Define the input and output file paths\n",
        "    input_file_path = f'/content/drive/MyDrive/EcoVerse_Results/{model_name}/formatted_results/results_{iteration}.txt'\n",
        "    output_file_path = f'/content/drive/MyDrive/EcoVerse_Results/{model_name}/JSON/output_{iteration}.json'\n",
        "\n",
        "    # Initialize a list to hold the extracted data\n",
        "    data_list = []\n",
        "\n",
        "    # Open and read the input file\n",
        "    with open(input_file_path, 'r', encoding='utf-8') as file:\n",
        "        for ID, line in zip(ids, file):\n",
        "            # Extract the required fields using regular expressions\n",
        "            id_match = re.search(r'ID: (\\d+)', line)\n",
        "            label_match = re.search(r'LABEL: eco-related: (YES|NO)', line)\n",
        "            prob_no_match = re.search(r'NO_PROB=([\\d\\.]+%?|\\d+%)', line)\n",
        "            prob_yes_match = re.search(r'YES_PROB=([\\d\\.]+%?|\\d+%)', line)\n",
        "            exp_match = re.search(r'EXP: (.+)$', line)\n",
        "            if prob_no_match:\n",
        "              p_no = normalize_probability(prob_no_match.group(1))\n",
        "            if prob_yes_match:\n",
        "              p_yes = normalize_probability(prob_yes_match.group(1))\n",
        "\n",
        "            # Check for matches and assign \"NA\" if no match is found\n",
        "            id_val = ID\n",
        "            label_val = 'eco-related' if label_match and label_match.group(1) == 'YES' else 'Not eco-related'\n",
        "            prob_no_val = str(p_no) if prob_no_match else (str(1.0 - p_yes) if prob_yes_match else \"0.0\")\n",
        "            prob_yes_val = str(p_yes) if prob_yes_match else (str(1.0 - p_no) if prob_no_match else \"0.0\")\n",
        "            exp_val = exp_match.group(1) if exp_match else \"NA\"\n",
        "\n",
        "            # Construct the dictionary for this line\n",
        "            data = {\n",
        "                'ID': id_val,\n",
        "                'isgreen': label_val,\n",
        "                'prob_yes': prob_yes_val,\n",
        "                'prob_no': prob_no_val,\n",
        "                'explanation': exp_val\n",
        "            }\n",
        "            data_list.append(data)\n",
        "\n",
        "    # Write the extracted data to a JSON file\n",
        "    with open(output_file_path, 'w') as json_file:\n",
        "        json.dump(data_list, json_file, indent=4)\n"
      ],
      "metadata": {
        "id": "ELDO8dLK3_IT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "\n",
        "for iteration in range(1, 101):\n",
        "    # Define the input and output file paths\n",
        "    input_file_path = f'/content/drive/MyDrive/EcoVerse_Results/{model_name}/eia/formatted_results/results_{iteration}.txt'\n",
        "    output_file_path = f'/content/drive/MyDrive/EcoVerse_Results/{model_name}/eia/JSON/output_{iteration}.json'\n",
        "\n",
        "    # Initialize a list to hold the extracted data\n",
        "    data_list = []\n",
        "\n",
        "    # Open and read the input file\n",
        "    with open(input_file_path, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            # Extract the required fields using regular expressions\n",
        "            id_match = re.search(r'ID: (\\d+)', line)\n",
        "            label_match = re.search(r'LABEL: (NEG|POS|NEU);', line)\n",
        "            prob_pos_match = re.search(r'POS_PROB=([\\d\\.]+%?|\\d+%)', line)\n",
        "            prob_pos_match = re.search(r'POS_PROB=([\\d\\.]+%?|\\d+%)', line)\n",
        "            prob_neu_match = re.search(r'NEU_PROB=([\\d\\.]+%?|\\d+%)', line)\n",
        "            prob_neg_match = re.search(r'NEG_PROB=([\\d\\.]+%?|\\d+%)', line)\n",
        "\n",
        "            exp_match = re.search(r'EXP: (.+)$', line)\n",
        "            if prob_pos_match:\n",
        "              p_pos = normalize_probability(prob_pos_match.group(1))\n",
        "            if prob_neu_match:\n",
        "              p_neu = normalize_probability(prob_neu_match.group(1))\n",
        "            if prob_neg_match:\n",
        "              p_neg = normalize_probability(prob_neg_match.group(1))\n",
        "\n",
        "\n",
        "            # Check for matches and assign \"NA\" if no match is found\n",
        "            id_val = id_match.group(1)\n",
        "            if label_match:\n",
        "              if label_match.group(1) == 'POS':\n",
        "                label_val = 'Positive'\n",
        "              elif label_match.group(1) == 'NEU':\n",
        "                label_val = 'Neutral'\n",
        "              else:\n",
        "                label_val = 'Negative'\n",
        "            prob_pos_val = str(p_pos) if prob_pos_match else \"NA\"\n",
        "            prob_neu_val = str(p_neu) if prob_neu_match else \"NA\"\n",
        "            prob_neg_val = str(p_neg) if prob_neg_match else \"NA\"\n",
        "\n",
        "            exp_val = exp_match.group(1) if exp_match else \"NA\"\n",
        "\n",
        "            # Construct the dictionary for this line\n",
        "            data = {\n",
        "                'ID': id_val,\n",
        "                'sentiment': label_val,\n",
        "                'prob_pos': prob_pos_val,\n",
        "                'prob_neu': prob_neu_val,\n",
        "                'prob_neg': prob_neg_val,\n",
        "                'explanation': exp_val\n",
        "            }\n",
        "            data_list.append(data)\n",
        "\n",
        "    # Write the extracted data to a JSON file\n",
        "    with open(output_file_path, 'w') as json_file:\n",
        "        json.dump(data_list, json_file, indent=4)\n"
      ],
      "metadata": {
        "id": "ACRSi9oFB28j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "\n",
        "for iteration in range(1, 101):\n",
        "    # Define the input and output file paths\n",
        "    input_file_path = f'/content/drive/MyDrive/EcoVerse_Results/{model_name}/stance/formatted_results/results_{iteration}.txt'\n",
        "    output_file_path = f'/content/drive/MyDrive/EcoVerse_Results/{model_name}/stance/JSON/output_{iteration}.json'\n",
        "\n",
        "    # Initialize a list to hold the extracted data\n",
        "    data_list = []\n",
        "\n",
        "    # Open and read the input file\n",
        "    with open(input_file_path, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            # Extract the required fields using regular expressions\n",
        "            id_match = re.search(r'ID: (\\d+)', line)\n",
        "            label_match = re.search(r'LABEL: (SKE|SUP|NEU);', line)\n",
        "            prob_pos_match = re.search(r'SUP_PROB=([\\d\\.]+%?|\\d+%)', line)\n",
        "            prob_neu_match = re.search(r'NEU_PROB=([\\d\\.]+%?|\\d+%)', line)\n",
        "            prob_neg_match = re.search(r'SKE_PROB=([\\d\\.]+%?|\\d+%)', line)\n",
        "\n",
        "            exp_match = re.search(r'EXP: (.+)$', line)\n",
        "            if prob_pos_match:\n",
        "              p_pos = normalize_probability(prob_pos_match.group(1))\n",
        "            if prob_neu_match:\n",
        "              p_neu = normalize_probability(prob_neu_match.group(1))\n",
        "            if prob_neg_match:\n",
        "              p_neg = normalize_probability(prob_neg_match.group(1))\n",
        "\n",
        "\n",
        "            # Check for matches and assign \"NA\" if no match is found\n",
        "            id_val = id_match.group(1)\n",
        "            if label_match:\n",
        "              if label_match.group(1) == 'SUP':\n",
        "                label_val = 'Supportive'\n",
        "              elif label_match.group(1) == 'NEU':\n",
        "                label_val = 'Neutral'\n",
        "              else:\n",
        "                label_val = 'Skeptical/Opposing'\n",
        "            prob_pos_val = str(p_pos) if prob_pos_match else \"NA\"\n",
        "            prob_neu_val = str(p_neu) if prob_neu_match else \"NA\"\n",
        "            prob_neg_val = str(p_neg) if prob_neg_match else \"NA\"\n",
        "\n",
        "            exp_val = exp_match.group(1) if exp_match else \"NA\"\n",
        "\n",
        "            # Construct the dictionary for this line\n",
        "            data = {\n",
        "                'ID': id_val,\n",
        "                'stance': label_val,\n",
        "                'prob_sup': prob_pos_val,\n",
        "                'prob_neu': prob_neu_val,\n",
        "                'prob_ske': prob_neg_val,\n",
        "                'explanation': exp_val\n",
        "            }\n",
        "            data_list.append(data)\n",
        "\n",
        "    # Write the extracted data to a JSON file\n",
        "    with open(output_file_path, 'w') as json_file:\n",
        "        json.dump(data_list, json_file, indent=4)\n"
      ],
      "metadata": {
        "id": "5S58WUwZVVR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_file_path = f'/content/drive/MyDrive/EcoVerse_Results/{model_name}/stance/JSON/output_100.json'\n",
        "\n",
        "# Load the JSON data from the file\n",
        "with open(output_file_path, 'r', encoding='utf-8') as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "# Count the number of elements in the JSON data\n",
        "num_elements = len(data)\n",
        "print(num_elements)"
      ],
      "metadata": {
        "id": "d5J0m6oT3qk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Path to your JSON file\n",
        "json_file_path = f'/content/drive/MyDrive/EcoVerse_Results/{model_name}/JSON/output_1.json'\n",
        "\n",
        "# Initialize a list to hold the IDs\n",
        "eco_related_ids = []\n",
        "\n",
        "# Load the JSON data from the file\n",
        "with open(json_file_path, 'r', encoding='utf-8') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "    # Assuming the data is a list of dictionaries, each with an 'ID' field\n",
        "    for item in data:\n",
        "        # Append the ID to the list, use a default value if ID is missing\n",
        "        is_eco_rel = item.get('isgreen') == 'eco-related'\n",
        "        if is_eco_rel:\n",
        "          eco_related_ids.append(int(item.get('ID')))\n",
        "\n",
        "print((eco_related_ids))\n",
        "\n",
        "# Now ids contains all the ID values from the JSON file"
      ],
      "metadata": {
        "id": "YuW5zLJIBiqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metrics"
      ],
      "metadata": {
        "id": "6ycP_lYfcvug"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eco-relevance"
      ],
      "metadata": {
        "id": "2ClSVqcd36zf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "#eco_rel_labels = df['isgreen'].tolist()\n",
        "#eia_labels = df['sentiment'].tolist()\n",
        "#stance_labels = df['stance'].tolist()\n",
        "\n",
        "eco_rel_labels = [e.lower() for e in eco_rel_labels]\n",
        "\n",
        "mod = 'gemma-2b-it'\n",
        "target_names=['not eco-related', 'eco-related']\n",
        "accuracy_list, precision_list, recall_list, f1_list = [], [], [], []\n",
        "\n",
        "\n",
        "for iteration in range(1, 11):\n",
        "  json_file_path = f'/content/drive/MyDrive/EcoVerse_Results/{mod}/JSON/output_{iteration}.json'\n",
        "  eco_rel_pred = []\n",
        "  with open(json_file_path, 'r', encoding='utf-8') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "    # Assuming the data is a list of dictionaries, each with an 'ID' field\n",
        "    for item in data:\n",
        "        # Append the ID to the list, use a default value if ID is missing\n",
        "          eco_rel_pred.append(item.get('isgreen').lower())\n",
        "    #print(classification_report(eco_rel_labels, eco_rel_pred, target_names=target_names))\n",
        "    report = classification_report(eco_rel_labels, eco_rel_pred, target_names=target_names, output_dict=True)\n",
        "    accuracy_list.append(report['accuracy'])\n",
        "    precision_list.append(report['not eco-related']['precision'])\n",
        "    recall_list.append(report['eco-related']['recall'])\n",
        "    f1_list.append(report['eco-related']['f1-score'])\n",
        "\n",
        "mean_accuracy = np.mean(accuracy_list)\n",
        "mean_precision = np.mean(precision_list)\n",
        "mean_recall = np.mean(recall_list)\n",
        "mean_f1 = np.mean(f1_list)\n",
        "\n",
        "print(f\"Mean Accuracy for model {mod} on Eco-Relevance task: {mean_accuracy}\")\n",
        "print(f\"Mean Precision for model {mod} on Eco-Relevance task: {mean_precision}\")\n",
        "print(f\"Mean Recall for model {mod} on Eco-Relevance task: {mean_recall}\")\n",
        "print(f\"Mean F1-Score for model {mod} on Eco-Relevance task: {mean_f1}\")\n",
        "#print(classification_report(y_true, y_pred, target_names=target_names))\n"
      ],
      "metadata": {
        "id": "uTnEl8nxsxpF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f9592df-b20c-49da-f042-96d93d3df1e0"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Accuracy for model gemma-2b-it on Eco-Relevance task: 0.49537953795379536\n",
            "Mean Precision for model gemma-2b-it on Eco-Relevance task: 0.550619777835605\n",
            "Mean Recall for model gemma-2b-it on Eco-Relevance task: 0.39166666666666666\n",
            "Mean F1-Score for model gemma-2b-it on Eco-Relevance task: 0.40335404683160797\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EIA"
      ],
      "metadata": {
        "id": "PFzoQmW63-5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "#eco_rel_labels = df['isgreen'].tolist()\n",
        "#eia_labels = df['sentiment'].tolist()\n",
        "#stance_labels = df['stance'].tolist()\n",
        "\n",
        "\n",
        "mod = model_name\n",
        "\n",
        "\n",
        "target_names=['Positive', 'Neutral', 'Negative']\n",
        "accuracy = []\n",
        "precision_pos_list, precision_neu_list, precision_neg_list = [], [], []\n",
        "recall_pos_list, recall_neu_list, recall_neg_list = [], [], []\n",
        "f1_pos_list, f1_neu_list, f1_neg_list = [], [], []\n",
        "precision_macro_list, recall_macro_list, f1_macro_list = [], [], []\n",
        "\n",
        "for iteration in range(1, 11):\n",
        "  json_file_path = f'/content/drive/MyDrive/EcoVerse_Results/{mod}/eia/JSON/output_{iteration}.json'\n",
        "  eia_pred = []\n",
        "  ids_pred = []\n",
        "  with open(json_file_path, 'r', encoding='utf-8') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "    # Assuming the data is a list of dictionaries, each with an 'ID' field\n",
        "    for item in data:\n",
        "      ids_pred.append(int(item.get('ID')))\n",
        "      eia_pred.append(item.get('sentiment').lower())\n",
        "\n",
        "    data_real = df[(df['id'].isin(ids_pred)) & (df['sentiment'].notna())]\n",
        "    rem_ids = data_real['id'].tolist()\n",
        "    eia_pred = []\n",
        "    for item in data:\n",
        "      if int(item.get('ID')) in rem_ids:\n",
        "        eia_pred.append(item.get('sentiment'))\n",
        "\n",
        "    eia_labels = data_real['sentiment'].dropna().tolist()\n",
        "\n",
        "    #print(classification_report(eia_labels, eia_pred, target_names=target_names))\n",
        "    report = classification_report(eia_labels, eia_pred, target_names=target_names, output_dict=True)\n",
        "    accuracy.append(report['accuracy'])\n",
        "    precision_pos_list.append(report['Positive']['precision'])\n",
        "    recall_pos_list.append(report['Positive']['recall'])\n",
        "    f1_pos_list.append(report['Positive']['f1-score'])\n",
        "\n",
        "    precision_neu_list.append(report['Neutral']['precision'])\n",
        "    recall_neu_list.append(report['Neutral']['recall'])\n",
        "    f1_neu_list.append(report['Neutral']['f1-score'])\n",
        "\n",
        "    precision_neg_list.append(report['Negative']['precision'])\n",
        "    recall_neg_list.append(report['Negative']['recall'])\n",
        "    f1_neg_list.append(report['Negative']['f1-score'])\n",
        "\n",
        "    precision_macro_list.append(report['macro avg']['precision'])\n",
        "    recall_macro_list.append(report['macro avg']['recall'])\n",
        "    f1_macro_list.append(report['macro avg']['f1-score'])\n",
        "\n",
        "\n",
        "\n",
        "mean_accuracy = np.mean(accuracy)\n",
        "\n",
        "mean_precision_pos = np.mean(precision_pos_list)\n",
        "mean_recall_pos = np.mean(recall_pos_list)\n",
        "mean_f1_pos = np.mean(f1_pos_list)\n",
        "\n",
        "mean_precision_neu = np.mean(precision_neu_list)\n",
        "mean_recall_neu = np.mean(recall_neu_list)\n",
        "mean_f1_neu = np.mean(f1_neu_list)\n",
        "\n",
        "mean_precision_neg = np.mean(precision_neg_list)\n",
        "mean_recall_neg = np.mean(recall_neg_list)\n",
        "mean_f1_neg = np.mean(f1_neg_list)\n",
        "\n",
        "    # Now, calculate the mean for each macro avg metric\n",
        "mean_precision_macro = np.mean(precision_macro_list)\n",
        "mean_recall_macro = np.mean(recall_macro_list)\n",
        "mean_f1_macro = np.mean(f1_macro_list)\n",
        "\n",
        "\n",
        "# Printing mean metrics for each label\n",
        "print(f\"Mean Accuracy for model {mod} on EIA task: {mean_accuracy}\\n\")\n",
        "\n",
        "print(f\"Mean Macro Avg Precision for model {mod} on EIA task: {mean_precision_macro}\")\n",
        "print(f\"Mean Macro Avg Recall for model {mod} on EIA task: {mean_recall_macro}\")\n",
        "print(f\"Mean Macro Avg F1-Score for model {mod} on EIA task: {mean_f1_macro}\")\n",
        "\n",
        "print(f\"Mean Precision for model {mod} on EIA task (Positive): {mean_precision_pos}\")\n",
        "print(f\"Mean Recall for model {mod} on EIA task (Positive): {mean_recall_pos}\")\n",
        "print(f\"Mean F1-Score for model {mod} on EIA task (Positive): {mean_f1_pos}\\n\")\n",
        "\n",
        "print(f\"Mean Precision for model {mod} on EIA task (Neutral): {mean_precision_neu}\")\n",
        "print(f\"Mean Recall for model {mod} on EIA task (Neutral): {mean_recall_neu}\")\n",
        "print(f\"Mean F1-Score for model {mod} on EIA task (Neutral): {mean_f1_neu}\\n\")\n",
        "\n",
        "print(f\"Mean Precision for model {mod} on EIA task (Negative): {mean_precision_neg}\")\n",
        "print(f\"Mean Recall for model {mod} on EIA task (Negative): {mean_recall_neg}\")\n",
        "print(f\"Mean F1-Score for model {mod} on EIA task (Negative): {mean_f1_neg}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJDRc2y4dBU6",
        "outputId": "3183281b-ba99-459b-b862-ae33f9b49c09"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Accuracy for model gpt-3.5-turbo-0125 on EIA task: 0.8406779661016948\n",
            "\n",
            "Mean Macro Avg Precision for model gpt-3.5-turbo-0125 on EIA task: 0.6855595068752963\n",
            "Mean Macro Avg Recall for model gpt-3.5-turbo-0125 on EIA task: 0.6668987168987168\n",
            "Mean Macro Avg F1-Score for model gpt-3.5-turbo-0125 on EIA task: 0.667537165590874\n",
            "Mean Precision for model gpt-3.5-turbo-0125 on EIA task (Positive): 0.8337837837837837\n",
            "Mean Recall for model gpt-3.5-turbo-0125 on EIA task (Positive): 0.9121212121212119\n",
            "Mean F1-Score for model gpt-3.5-turbo-0125 on EIA task (Positive): 0.8711801242236025\n",
            "\n",
            "Mean Precision for model gpt-3.5-turbo-0125 on EIA task (Neutral): 0.34\n",
            "Mean Recall for model gpt-3.5-turbo-0125 on EIA task (Neutral): 0.18181818181818182\n",
            "Mean F1-Score for model gpt-3.5-turbo-0125 on EIA task (Neutral): 0.23676470588235293\n",
            "\n",
            "Mean Precision for model gpt-3.5-turbo-0125 on EIA task (Negative): 0.8828947368421053\n",
            "Mean Recall for model gpt-3.5-turbo-0125 on EIA task (Negative): 0.9067567567567567\n",
            "Mean F1-Score for model gpt-3.5-turbo-0125 on EIA task (Negative): 0.8946666666666665\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stance"
      ],
      "metadata": {
        "id": "qtaN63jVWt8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "#eco_rel_labels = df['isgreen'].tolist()\n",
        "#eia_labels = df['sentiment'].tolist()\n",
        "#stance_labels = df['stance'].tolist()\n",
        "\n",
        "\n",
        "#mod = 'Llama-2-13b-chat-hf'\n",
        "#mod = 'gemma-2b-it'\n",
        "mod = 'gpt-3.5-turbo-0125'\n",
        "\n",
        "target_names=['Supportive', 'Neutral', 'Skeptical/Opposing']\n",
        "accuracy = []\n",
        "precision_pos_list, precision_neu_list, precision_neg_list = [], [], []\n",
        "recall_pos_list, recall_neu_list, recall_neg_list = [], [], []\n",
        "f1_pos_list, f1_neu_list, f1_neg_list = [], [], []\n",
        "precision_macro_list, recall_macro_list, f1_macro_list = [], [], []\n",
        "\n",
        "for iteration in range(1, 11):\n",
        "  json_file_path = f'/content/drive/MyDrive/EcoVerse_Results/{mod}/stance/JSON/output_{iteration}.json'\n",
        "  stance_pred = []\n",
        "  ids_pred = []\n",
        "  with open(json_file_path, 'r', encoding='utf-8') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "    # Assuming the data is a list of dictionaries, each with an 'ID' field\n",
        "    for item in data:\n",
        "      ids_pred.append(int(item.get('ID')))\n",
        "      stance_pred.append(item.get('stance').lower())\n",
        "\n",
        "    data_real = df[(df['id'].isin(ids_pred)) & (df['stance'].notna())]\n",
        "    rem_ids = data_real['id'].tolist()\n",
        "    stance_pred = []\n",
        "    for item in data:\n",
        "      if int(item.get('ID')) in rem_ids:\n",
        "        stance_pred.append(item.get('stance'))\n",
        "\n",
        "    stance_labels = data_real['stance'].dropna().tolist()\n",
        "\n",
        "    #print(classification_report(eco_rel_labels, eco_rel_pred, target_names=target_names))\n",
        "    report = classification_report(stance_labels, stance_pred, target_names=target_names, output_dict=True)\n",
        "    accuracy.append(report['accuracy'])\n",
        "    precision_pos_list.append(report['Supportive']['precision'])\n",
        "    recall_pos_list.append(report['Supportive']['recall'])\n",
        "    f1_pos_list.append(report['Supportive']['f1-score'])\n",
        "\n",
        "    precision_neu_list.append(report['Neutral']['precision'])\n",
        "    recall_neu_list.append(report['Neutral']['recall'])\n",
        "    f1_neu_list.append(report['Neutral']['f1-score'])\n",
        "\n",
        "    precision_neg_list.append(report['Skeptical/Opposing']['precision'])\n",
        "    recall_neg_list.append(report['Skeptical/Opposing']['recall'])\n",
        "    f1_neg_list.append(report['Skeptical/Opposing']['f1-score'])\n",
        "\n",
        "    precision_macro_list.append(report['macro avg']['precision'])\n",
        "    recall_macro_list.append(report['macro avg']['recall'])\n",
        "    f1_macro_list.append(report['macro avg']['f1-score'])\n",
        "\n",
        "\n",
        "\n",
        "mean_accuracy = np.mean(accuracy)\n",
        "\n",
        "mean_precision_pos = np.mean(precision_pos_list)\n",
        "mean_recall_pos = np.mean(recall_pos_list)\n",
        "mean_f1_pos = np.mean(f1_pos_list)\n",
        "\n",
        "mean_precision_neu = np.mean(precision_neu_list)\n",
        "mean_recall_neu = np.mean(recall_neu_list)\n",
        "mean_f1_neu = np.mean(f1_neu_list)\n",
        "\n",
        "mean_precision_neg = np.mean(precision_neg_list)\n",
        "mean_recall_neg = np.mean(recall_neg_list)\n",
        "mean_f1_neg = np.mean(f1_neg_list)\n",
        "\n",
        "    # Now, calculate the mean for each macro avg metric\n",
        "mean_precision_macro = np.mean(precision_macro_list)\n",
        "mean_recall_macro = np.mean(recall_macro_list)\n",
        "mean_f1_macro = np.mean(f1_macro_list)\n",
        "\n",
        "\n",
        "# Printing mean metrics for each label\n",
        "print(f\"Mean Accuracy for model {mod} on stance task: {mean_accuracy}\\n\")\n",
        "\n",
        "print(f\"Mean Macro Avg Precision for model {mod} on stance task: {mean_precision_macro}\")\n",
        "print(f\"Mean Macro Avg Recall for model {mod} on stance task: {mean_recall_macro}\")\n",
        "print(f\"Mean Macro Avg F1-Score for model {mod} on stance task: {mean_f1_macro}\")\n",
        "\n",
        "print(f\"Mean Precision for model {mod} on EIA task (Positive): {mean_precision_pos}\")\n",
        "print(f\"Mean Recall for model {mod} on EIA task (Positive): {mean_recall_pos}\")\n",
        "print(f\"Mean F1-Score for model {mod} on EIA task (Positive): {mean_f1_pos}\\n\")\n",
        "\n",
        "print(f\"Mean Precision for model {mod} on EIA task (Neutral): {mean_precision_neu}\")\n",
        "print(f\"Mean Recall for model {mod} on EIA task (Neutral): {mean_recall_neu}\")\n",
        "print(f\"Mean F1-Score for model {mod} on EIA task (Neutral): {mean_f1_neu}\\n\")\n",
        "\n",
        "print(f\"Mean Precision for model {mod} on EIA task (Negative): {mean_precision_neg}\")\n",
        "print(f\"Mean Recall for model {mod} on EIA task (Negative): {mean_recall_neg}\")\n",
        "print(f\"Mean F1-Score for model {mod} on EIA task (Negative): {mean_f1_neg}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bXFT6GEWtjo",
        "outputId": "92df8b1f-30d8-414c-b683-e983f945800e"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Accuracy for model gpt-3.5-turbo-0125 on stance task: 0.6587301587301588\n",
            "\n",
            "Mean Macro Avg Precision for model gpt-3.5-turbo-0125 on stance task: 0.5684678115807869\n",
            "Mean Macro Avg Recall for model gpt-3.5-turbo-0125 on stance task: 0.7065733955141018\n",
            "Mean Macro Avg F1-Score for model gpt-3.5-turbo-0125 on stance task: 0.5628044053136032\n",
            "Mean Precision for model gpt-3.5-turbo-0125 on EIA task (Positive): 0.5583388962084614\n",
            "Mean Recall for model gpt-3.5-turbo-0125 on EIA task (Positive): 0.3526315789473684\n",
            "Mean F1-Score for model gpt-3.5-turbo-0125 on EIA task (Positive): 0.43148249484488294\n",
            "\n",
            "Mean Precision for model gpt-3.5-turbo-0125 on EIA task (Neutral): 0.2873995244270223\n",
            "Mean Recall for model gpt-3.5-turbo-0125 on EIA task (Neutral): 1.0\n",
            "Mean F1-Score for model gpt-3.5-turbo-0125 on EIA task (Neutral): 0.4462703171384546\n",
            "\n",
            "Mean Precision for model gpt-3.5-turbo-0125 on EIA task (Negative): 0.8596650141068768\n",
            "Mean Recall for model gpt-3.5-turbo-0125 on EIA task (Negative): 0.7670886075949367\n",
            "Mean F1-Score for model gpt-3.5-turbo-0125 on EIA task (Negative): 0.8106604039574721\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calibration"
      ],
      "metadata": {
        "id": "E0R1X8OtrflH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cal Eco-Relevance"
      ],
      "metadata": {
        "id": "01U52UI4riEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFCkSMdnp1MV",
        "outputId": "68a87a58-4f2d-49e4-8404-768e62e99a11"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.4.1.post1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scipy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3OsXMs3q0pS",
        "outputId": "fdb26967-049d-4e0b-ec8b-7004d73cb6a0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.25.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score\n",
        "from scipy.stats import binned_statistic\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "# Initialize lists for probabilities and labels\n",
        "eco_rel_probs = []\n",
        "eco_rel_labels = []  # Assuming binary labels (0 for 'not eco-related', 1 for 'eco-related')\n",
        "\n",
        "# Example loop to populate lists - replace with actual data loading as needed\n",
        "for iteration in range(1, 11):\n",
        "    json_file_path = f'/content/drive/MyDrive/EcoVerse_Results/{model_name}/JSON/output_{iteration}.json'\n",
        "    with open(json_file_path, 'r', encoding='utf-8') as file:\n",
        "        data = json.load(file)\n",
        "        for item in data:\n",
        "            eco_rel_probs.append(float(item.get('prob_no')))\n",
        "            eco_rel_labels.append(1 if item.get('isgreen').lower() == 'not eco-related' else 0)\n",
        "\n",
        "# Convert lists to numpy arrays for efficient computation\n",
        "eco_rel_probs = np.array(eco_rel_probs)\n",
        "eco_rel_labels = np.array(eco_rel_labels)\n",
        "\n",
        "# Define bins and bin labels for probability ranges\n",
        "bins = [0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
        "bin_labels = ['0-0.2', '0.2-0.4', '0.4-0.6', '0.6-0.8', '0.8-1.0']\n",
        "\n",
        "# Digitize the probabilities into bins\n",
        "digitized_probs = np.digitize(eco_rel_probs, bins) - 1\n",
        "\n",
        "# Calculate mean probability and precision for each bin\n",
        "for i in range(len(bins)-1):\n",
        "    bin_indices = (digitized_probs == i)\n",
        "    if bin_indices.any():\n",
        "        bin_mean_prob = np.mean(eco_rel_probs[bin_indices])\n",
        "        bin_precision = precision_score(eco_rel_labels[bin_indices], eco_rel_probs[bin_indices] >= 0.5)\n",
        "        print(f\"Bin {bin_labels[i]} - Mean Probability: {bin_mean_prob}, Precision: {bin_precision}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgtGxq6CqCAK",
        "outputId": "ad3abc5e-474c-4bbf-a4d9-058f85bf82c0"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bin 0-0.2 - Mean Probability: 0.03632450157397692, Precision: 0.0\n",
            "Bin 0.2-0.4 - Mean Probability: 0.2517596566523605, Precision: 0.0\n",
            "Bin 0.4-0.6 - Mean Probability: 0.502, Precision: 0.2\n",
            "Bin 0.6-0.8 - Mean Probability: 0.7443655172413793, Precision: 0.8620689655172413\n",
            "Bin 0.8-1.0 - Mean Probability: 0.9395314285714285, Precision: 0.32571428571428573\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cal eia"
      ],
      "metadata": {
        "id": "RkuftVN05wQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score\n",
        "from scipy.stats import binned_statistic\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "# Initialize lists for probabilities and labels\n",
        "sentiment_probs = []\n",
        "sentiment_labels = []  # Assuming binary labels (0 for 'not eco-related', 1 for 'eco-related')\n",
        "\n",
        "# Example loop to populate lists - replace with actual data loading as needed\n",
        "for iteration in range(1, 101):\n",
        "    json_file_path = f'/content/drive/MyDrive/EcoVerse_Results/{model_name}/eia/JSON/output_{iteration}.json'\n",
        "    with open(json_file_path, 'r', encoding='utf-8') as file:\n",
        "        data = json.load(file)\n",
        "        for item in data:\n",
        "            if item.get('prob_neg') is not None and item.get('prob_neg') != \"NA\":\n",
        "              sentiment_probs.append(float(item.get('prob_neg')))\n",
        "              sentiment_labels.append(1 if item.get('sentiment').lower() == 'neg' else 0)\n",
        "# Convert lists to numpy arrays for efficient computation\n",
        "sentiment_probs = np.array(sentiment_probs)\n",
        "sentiment_labels = np.array(sentiment_labels)\n",
        "# Define bins and bin labels for probability ranges\n",
        "bins = [0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
        "bin_labels = ['0-0.2', '0.2-0.4', '0.4-0.6', '0.6-0.8', '0.8-1.0']\n",
        "\n",
        "# Digitize the probabilities into bins\n",
        "digitized_probs = np.digitize(sentiment_probs, bins) - 1\n",
        "\n",
        "# Calculate mean probability and precision for each bin\n",
        "for i in range(len(bins)-1):\n",
        "    bin_indices = (digitized_probs == i)\n",
        "    if bin_indices.any():\n",
        "        bin_mean_prob = np.mean(sentiment_probs[bin_indices])\n",
        "        bin_precision = precision_score(sentiment_labels[bin_indices], sentiment_probs[bin_indices] >= 0.5)\n",
        "        print(f\"Bin {bin_labels[i]} - Mean Probability: {bin_mean_prob}, Precision: {bin_precision}\")"
      ],
      "metadata": {
        "id": "_C79cVAv5I6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import json\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "bins = np.linspace(0, 1, 6)\n",
        "bin_labels = ['0-0.2', '0.2-0.4', '0.4-0.6', '0.6-0.8', '0.8-1.0']\n",
        "sentiments = ['pos', 'neu', 'neg']\n",
        "\n",
        "precision_bins = {sentiment: {label: [] for label in bin_labels} for sentiment in sentiments}\n",
        "bin_mean_probs = {sentiment: {label: [] for label in bin_labels} for sentiment in sentiments}\n",
        "\n",
        "for iteration in range(1, 11):\n",
        "    json_file_path = f'/content/drive/MyDrive/EcoVerse_Results/{model_name}/eia/JSON/output_{iteration}.json'\n",
        "    with open(json_file_path, 'r', encoding='utf-8') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    # Initialize or clear lists for predicted labels and probabilities for this iteration\n",
        "    eia_pred, pos_probs, neu_probs, neg_probs = [], [], [], []\n",
        "    for item in data:\n",
        "        sentiment_pred = item.get('sentiment').lower()\n",
        "        eia_pred.append(sentiment_pred)\n",
        "        pos_probs.append(float(item.get('prob_pos')))\n",
        "        neu_probs.append(float(item.get('prob_neu')))\n",
        "        neg_probs.append(float(item.get('prob_neg')))\n",
        "\n",
        "    # Assume eia_labels are loaded with the correct labels for this iteration\n",
        "    eia_labels_lower = [label.lower() for label in eia_labels]  # Ensure lowercase for consistency\n",
        "    for sentiment in sentiments:\n",
        "        sentiment_probs = {\n",
        "            'pos': pos_probs,\n",
        "            'neu': neu_probs,\n",
        "            'neg': neg_probs\n",
        "        }[sentiment]\n",
        "\n",
        "        bin_indices = np.digitize(sentiment_probs, bins) - 1  # Assign probabilities to bins\n",
        "\n",
        "        for i, bin_range in enumerate(bin_labels):\n",
        "            in_bin = bin_indices == i\n",
        "            if np.any(in_bin):\n",
        "                # Filter predictions and true labels for current bin\n",
        "                bin_preds = [eia_pred[j] for j in range(len(eia_pred)) if in_bin[j]]\n",
        "                bin_true = [eia_labels_lower[j] for j in range(len(eia_labels_lower)) if in_bin[j]]\n",
        "\n",
        "                # Binarize labels for precision calculation\n",
        "                bin_preds_binarized = label_binarize(bin_preds, classes=sentiments)[:, sentiments.index(sentiment)]\n",
        "                bin_true_binarized = label_binarize(bin_true, classes=sentiments)[:, sentiments.index(sentiment)]\n",
        "\n",
        "                precision = precision_score(bin_true_binarized, bin_preds_binarized, zero_division=0)\n",
        "                precision_bins[sentiment][bin_range].append(precision)\n",
        "                bin_mean_probs[sentiment][bin_range].append(np.mean(sentiment_probs))\n",
        "\n",
        "# Calculate and print mean precision and probability for each bin and sentiment\n",
        "for sentiment in sentiments:\n",
        "    print(f\"\\nSentiment: {sentiment.capitalize()}\")\n",
        "    for bin_range in bin_labels:\n",
        "        mean_precision = np.mean(precision_bins[sentiment][bin_range], default=0)\n",
        "        mean_prob = np.mean(bin_mean_probs[sentiment][bin_range], default=0)\n",
        "        print(f\"Bin {bin_range}, Mean Probability: {mean_prob:.4f} - Mean Precision: {mean_precision:.4f}\")"
      ],
      "metadata": {
        "id": "F3-YeJgJ5v65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## cal stance"
      ],
      "metadata": {
        "id": "vBJKcja345iD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score\n",
        "from scipy.stats import binned_statistic\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "# Initialize lists for probabilities and labels\n",
        "sentiment_probs = []\n",
        "sentiment_labels = []  # Assuming binary labels (0 for 'not eco-related', 1 for 'eco-related')\n",
        "\n",
        "# Example loop to populate lists - replace with actual data loading as needed\n",
        "for iteration in range(1, 11):\n",
        "    json_file_path = f'/content/drive/MyDrive/EcoVerse_Results/{model_name}/stance/JSON/output_{iteration}.json'\n",
        "    with open(json_file_path, 'r', encoding='utf-8') as file:\n",
        "        data = json.load(file)\n",
        "        for item in data:\n",
        "            if item.get('prob_neu') is not None and item.get('prob_neu') != \"NA\":\n",
        "              sentiment_probs.append(float(item.get('prob_neu')))\n",
        "              sentiment_labels.append(1 if item.get('stance').lower() == 'neu' else 0)\n",
        "# Convert lists to numpy arrays for efficient computation\n",
        "sentiment_probs = np.array(sentiment_probs)\n",
        "sentiment_labels = np.array(sentiment_labels)\n",
        "# Define bins and bin labels for probability ranges\n",
        "bins = [0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
        "bin_labels = ['0-0.2', '0.2-0.4', '0.4-0.6', '0.6-0.8', '0.8-1.0']\n",
        "\n",
        "# Digitize the probabilities into bins\n",
        "digitized_probs = np.digitize(sentiment_probs, bins) - 1\n",
        "\n",
        "# Calculate mean probability and precision for each bin\n",
        "for i in range(len(bins)-1):\n",
        "    bin_indices = (digitized_probs == i)\n",
        "    if bin_indices.any():\n",
        "        bin_mean_prob = np.mean(sentiment_probs[bin_indices])\n",
        "        bin_precision = precision_score(sentiment_labels[bin_indices], sentiment_probs[bin_indices] >= 0.5)\n",
        "        print(f\"Bin {bin_labels[i]} - Mean Probability: {bin_mean_prob}, Precision: {bin_precision}\")"
      ],
      "metadata": {
        "id": "IwnNLtIq4hcS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}